# Data engineer portfolio

---

### Pet-projects Description

| Project name                  | Description                                                                                                                                         | 	Skills and tools                                                                                                              |
|:------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|
| [*Data warehouse*][1]         | Design `DWH` and describe the subject area                                                                                                          | `Inmon` `Kimball` `Data volt2` `Anchor modeling`                                                                               |
| [*Hadoop HDFS map reduce*][2] | MapReduce task for aggregating data about New York Taxi using `Hadoop HDFS` infrastructure                                                          | `Yandex.Cloud` `HadoopStreaming` `HDFS` `S3` `MapReduce` `CLI` `Shell` `Hadoop Cluster Administration` `ETL`                   |
| [*Hadoop Hive*][3]            | Providing constant access to cold data, creating a Star scheme and a showcase using the `Hadoop Hive` infrastructure                                | `Yandex.Cloud` `S3` `HDFS` `HIVE` `MapReduce` `TEZ` `YARN` `HiveSQL` `CLI` `Shell` `Hadoop Cluster Administration`             |
| [*Apache Spark*][4]           | Creating a data showcase using the `Apache Spark` infrastructure                                                                                    | `Yandex.Cloud` `S3` `HDFS` `PySpark` `CLI` `Shell` `Hadoop Cluster Administration`                                             |
| [*Docker Kafka Spark*][5]     | Creating a data showcase using the `Docker-compose`, `Kafka`, `GreenPlum` and `Spark` infrastructure                                                | `Yandex.Cloud` `Kafka` `SparkStreaming` `Docker-compose` `ZooKeeper` `GreenPlum`                                               |
| [*Apache Airflow*][6]         | Auto-collecting of currency exchange rate data from the website with `Apache Kafka` and uploading to `GreenPlum`                                    | `VK.Cloud` `Airflow` `GreenPlum` `Jinja` `macros` `ETL` `parsing` `bash` `IDE` `CI/CD`                                         |
| [*Google Kubernetes*][7]      | Deploying a `Kubernetes` cluster with the installation of components to run the custom script and tracking the result using `Spring History Server` | `VK.Cloud` `terminal` `Ubuntu` `Kubectl` `Kubernetes` `Helm` `DOCKER` `S3` `Spark` `Spark Operator`  `Spark History Server`    |
| [*Apache SparkML*][8]         | Creating a bot identifier using `PySpark` among user sessions with two tasks - to train the best data model and to apply it.                        | `PySpark` `SparkML` `SparkSession` `Pipeline`                                                                                  |
| [*Docker PostgreSQL*][9]      | Initiate `Docker-compose` container with `PostgreSQL`                        | `Docker` `Docker-compose` `PostgreSQL` `Adminer` `Python` `Pscorpg2`                                                                                  |


[1]:https://github.com/Amboss/portfolio_projects/tree/master/data_warehouse
[2]:https://github.com/Amboss/portfolio_projects/tree/master/hadoop_mapreduce
[3]:https://github.com/Amboss/portfolio_projects/tree/master/hadoop_hive
[4]:https://github.com/Amboss/portfolio_projects/tree/master/apache_spark
[5]:https://github.com/Amboss/portfolio_projects/tree/master/docker_kafka_spark
[6]:https://github.com/Amboss/apache_airflow
[7]:https://github.com/Amboss/portfolio_projects/tree/master/google_kubernetes
[8]:https://github.com/Amboss/portfolio_projects/tree/master/apache_pyspark_ml
[9]:https://github.com/Amboss/docker_postgres_python
