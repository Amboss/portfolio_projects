# Data engineer portfolio

---

### Pet-projects Description

| Project name                  | Description                                                                                                                                           | 	Skills and tools                                                                                                             |
|:------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|
| [*Hadoop HDFS map reduce*][1] | MapReduce task for aggregating data about New York Taxi using `Hadoop HDFS` infrastructure                                                            | `Yandex.Cloud` `HadoopStreaming` `HDFS` `S3` `MapReduce` `CLI` `Shell` `Hadoop Cluster Administration` `ETL`                  |
| [*Hadoop Hive*][2]            | Providing constant access to cold data, creating a Star scheme and a showcase using the `Hadoop Hive` infrastructure                                  | `Yandex.Cloud` `S3` `HDFS` `HIVE` `MapReduce` `TEZ` `YARN` `HiveSQL` `CLI` `Shell` `Hadoop Cluster Administration`            |
| [*Apache Spark*][3]           | Creating a data showcase using the `Apache Spark` infrastructure                                                                                      | `Yandex.Cloud` `S3` `HDFS` `PySpark` `CLI` `Shell` `Hadoop Cluster Administration`                                            |
| [*Docker Kafka Spark*][4]     | Creating a data showcase using the `Docker-compose`, `Kafka`, `GreenPlum` and `Spark` infrastructure                                                  | `Yandex.Cloud` `Kafka` `SparkStreaming` `Docker-compose` `ZooKeeper` `GreenPlum`                                              |
| [*Apache Airflow*][5]         | Auto-collecting of currency exchange rate data from the website with `Apache Kafka` and uploading to `GreenPlum`                                      | `VK.Cloud` `Airflow` `GreenPlum` `Jinja` `macros` `ETL` `parsing` `bash` `IDE` `CI/CD`                                        |
| [*Google Kubernetes*][6]      | Deploying a `Kubernetes` cluster with the installation of components to run the custom script and tracking the result using `Spring History Server`   | `VK.Cloud` `terminal` `Ubuntu` `Kubectl` `Kubernetes` `Helm` `DOCKER` `S3` `Spark` `Spark Operator`  `Spark History Server`   |


[1]:https://github.com/Amboss/portfolio_projects/tree/master/hadoop_mapreduce
[2]:https://github.com/Amboss/portfolio_projects/tree/master/hadoop_hive
[3]:https://github.com/Amboss/portfolio_projects/tree/master/apache_spark
[4]:https://github.com/Amboss/portfolio_projects/tree/master/docker_kafka_spark
[5]:https://github.com/Amboss/portfolio_projects/tree/master/apache_airflow
[6]:https://github.com/Amboss/portfolio_projects/tree/master/google_kubernetes
